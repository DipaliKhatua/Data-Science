{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bb2c519",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\GitHub\\End to End Data Science Project\\4. Persona_Pulse\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b43bbabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2c52ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21df75e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all users from data\n",
    "def load_user_data(user_id):\n",
    "    with open(\"data/users.json\", \"r\") as f:\n",
    "        users = json.load(f)\n",
    "    for user in users:\n",
    "        if user[\"id\"] == user_id:\n",
    "            return user\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "101d9bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'neo03',\n",
       " 'name': 'Neo Noir',\n",
       " 'bio': 'Film noir buff with a taste for conspiracy theories and cold brew.',\n",
       " 'interests': ['Film Noir', 'History', 'Dark Comedy'],\n",
       " 'writing_samples': ['Everyone has a story √¢‚Ç¨‚Äù mine√¢‚Ç¨‚Ñ¢s written in shadows and static.']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_user_data(\"neo03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8c483fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt based on the user‚Äôs personality\n",
    "def generate_persona_prompt(user, message):\n",
    "    intro = f\"You are {user['name']}, a person who loves {', '.join(user['interests'])}.\\n\"\n",
    "    bio = f\"Bio: {user['bio']}\\n\"\n",
    "    writing_style = \"\\n\".join(user[\"writing_samples\"])\n",
    "    prompt = (\n",
    "        f\"{intro}{bio}Here are some things you‚Äôve said:\\n\"\n",
    "        f\"{writing_style}\\n\\n\"\n",
    "        f\"Now respond to the following message in your style:\\n\"\n",
    "        f\"User: {message}\\n{user['name']}:\"\n",
    "    )\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "885a4a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate AI response using the GPT-2 model\n",
    "def get_ai_reply(user_id, message):\n",
    "    user = load_user_data(user_id)\n",
    "    if not user:\n",
    "        return \"User not found.\"\n",
    "\n",
    "    prompt = generate_persona_prompt(user, message)\n",
    "\n",
    "    # Tokenize the input prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate the output with a limit of 150 tokens\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=150,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        no_repeat_ngram_size=2,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "\n",
    "    # Decode the output and extract the response part\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract only the part after the prompt\n",
    "    return response[len(prompt):].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c937da6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\GitHub\\End to End Data Science Project\\4. Persona_Pulse\\venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "e:\\GitHub\\End to End Data Science Project\\4. Persona_Pulse\\venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ AI Reply: I think poetry is a\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    reply = get_ai_reply(user_id=1, message=\"What's your opinion on AI and poetry?\")\n",
    "    print(\"ü§ñ AI Reply:\", reply)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
