{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45ad8192",
   "metadata": {},
   "source": [
    "Backpropagation (short for \"Backward Propagation of Errors\") is an optimization technique used in deep learning to train neural networks efficiently. It updates the network's weights based on the errors from its predictions, helping the model learn better over time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd438e12",
   "metadata": {},
   "source": [
    "#### What it does?\n",
    "- Purpose: It minimizes the cost (error) by adjusting weights and biases in the network.\n",
    "#### How It Works:- It calculates the error (loss) between predicted and actual values.\n",
    "- The error is propagated back through the network, layer by layer, using partial derivatives.\n",
    "- Adjustments are made to the weights to reduce the error in future predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c01836f",
   "metadata": {},
   "source": [
    "Steps in Backpropagation\n",
    "- 1. Forward Pass:- Input flows through the network to produce predictions (output).\n",
    "- Example: Predicting y = 10 when actual y = 12.\n",
    "\n",
    "- 2. Calculate Error:- Compute the difference (error) between predictions and actual values using a cost function.\n",
    "- Example: Mean Squared Error (MSE): $$\\text{Error} = (12 - 10)^2 = 4$$\n",
    "\n",
    "- 3. Backward Pass:- The error is sent backward through the network (from output to input) using the chain rule of derivatives.\n",
    "- Gradients are calculated to determine how much each weight contributes to the error.\n",
    "\n",
    "- 4. Update Weights:- Gradients are used to adjust weights and biases using an optimization algorithm (e.g., Gradient Descent).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573d5b49",
   "metadata": {},
   "source": [
    "Why Backpropagation is Needed\n",
    "- Efficiency: It updates weights systematically across all layers of the network.\n",
    "- Scalability: Handles complex, multi-layer architectures.\n",
    "- Accuracy: Helps reduce error and improve predictions over multiple iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5424aee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import Required Libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94d7945b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define Activation Function and Derivativ\n",
    "def sigmoid(x):\n",
    "    # Sigmoid activation function: 1 / (1 + e^(-x))\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    # Derivative of sigmoid for backpropagation\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1c13e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Initialize Training Data (Dummy)\n",
    "# Input features (2 features per example) means 2 columns 4 rows\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "\n",
    "# Target outputs (expected results for XOR problem)\n",
    "y = np.array([[0], [1], [1], [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ecbdd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Initialize Weights and Bias\n",
    "# Random weights initialization\n",
    "np.random.seed(42)  # For consistent results\n",
    "weights_input_hidden = np.random.rand(2, 2)  # Between input and hidden layer\n",
    "weights_hidden_output = np.random.rand(2, 1)  # Between hidden and output layer\n",
    "bias_hidden = np.random.rand(1, 2)  # Bias for hidden layer\n",
    "bias_output = np.random.rand(1, 1)  # Bias for output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "087bc615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Implement Forward and Backward Pass\n",
    "\n",
    "# Training process\n",
    "for epoch in range(10000):  # Number of iterations\n",
    "    # Forward Pass\n",
    "    # Step 1: Input to Hidden Layer\n",
    "    hidden_layer_input = np.dot(X, weights_input_hidden) + bias_hidden\n",
    "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "\n",
    "    # Step 2: Hidden to Output Layer\n",
    "    output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + bias_output\n",
    "    predicted_output = sigmoid(output_layer_input)\n",
    "\n",
    "    # Backward Pass (Error Calculation and Weight Update)\n",
    "    # Step 3: Calculate Error\n",
    "    error = y - predicted_output\n",
    "\n",
    "    # Step 4: Compute Gradients\n",
    "    output_gradient = error * sigmoid_derivative(predicted_output)\n",
    "    hidden_error = output_gradient.dot(weights_hidden_output.T)\n",
    "    hidden_gradient = hidden_error * sigmoid_derivative(hidden_layer_output)\n",
    "\n",
    "    # Step 5: Update Weights and Biases\n",
    "    weights_hidden_output += hidden_layer_output.T.dot(output_gradient) * 0.1  # Learning rate = 0.1\n",
    "    weights_input_hidden += X.T.dot(hidden_gradient) * 0.1\n",
    "    bias_hidden += np.sum(hidden_gradient, axis=0, keepdims=True) * 0.1\n",
    "    bias_output += np.sum(output_gradient, axis=0, keepdims=True) * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b0de2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Predictions:\n",
      "[[0.06029012]\n",
      " [0.94447222]\n",
      " [0.944367  ]\n",
      " [0.05997169]]\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Test the Model\n",
    "# Print final predictions\n",
    "print(\"Final Predictions:\")\n",
    "print(predicted_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
